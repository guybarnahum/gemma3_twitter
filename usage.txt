Usage:
  $0 setup                          # create venv and install requirements
  $0 shell                          # open a subshell with venv activated

  $0 convert ARCHIVE [OUT]          # convert Twitter/X archive → JSONL (default OUT={{DEFAULT_OUT}})
  $0 sync [args...]                 # incremental tweet sync (pass args through or use .env)

  $0 docs_sync [--path PATH] [--out OUT] [--state STATE]
           [--mode style|qa_heading]
           [--min_chars N] [--max_chars N]
           [--lang_hint "TEXT"]
           [--dedup-dataset] [--delete-missing]

  $0 sql_threads --input forum.sql|forum.db --nick YOUR_NICK
           [--out {{DEFAULT_OUT}}] [--max_context K]
           [--strip_self_context]
           [--role_assistant assistant|model]

  $0 unify [OUT] [--in FILE ...] [--shuffle] [--seed N]
           # If no --in given, auto-scans dataset/*.jsonl (recursively),
           # excludes train_eval.jsonl, the OUT file itself, and empty files.

  $0 train [args...]                # run {{TRAIN_SCRIPT}} (args forwarded)
  $0 daily                          # sync (from .env) → train (auto-resume)
  $0 infer [BASE] [ADAPTER] [PROMPT]        # read prompt from stdin if omitted
  $0 merge_adapter [BASE] [ADAPTER] [MERGED_DIR]
  $0 infer_merged [MERGED_DIR] [PROMPT]     # read prompt from stdin if omitted
  $0 clean                          # remove venv

Examples:
  $0 setup
  $0 convert ~/Downloads/twitter-archive {{DEFAULT_OUT}}
  $0 sync --username your_handle --out {{DEFAULT_OUT}} --state {{STATE_FILE}}
  $0 docs_sync --path docs/ --out {{DEFAULT_OUT}} --state state/docs_sync.json --mode style --dedup-dataset
  $0 sql_threads --input forum.db --nick "YourNick" --out dataset/forum.jsonl --max_context 6
  $0 unify                           # auto-scan dataset/*.jsonl → {{DEFAULT_OUT}}
  $0 unify dataset/my_train.jsonl    # choose a different output
  $0 unify --seed 7                  # pass options to the unifier
  echo "Write a concise tweet about: SLAM vs LiDAR." | $0 infer

Defaults (overridable via .env):
  MODEL_NAME=google/gemma-3-4b-it
  ADAPTER_DIR=out/gemma3-twitter-lora
  MERGED_DIR=out/gemma3-merged
  EPOCHS=1

Env (.env) keys (optional unless noted):
  # --- Twitter API / sync ---
  TWITTER_BEARER_TOKEN   # required for 'sync' unless passed via --bearer
  TWITTER_USERNAME       # your handle without '@' (for 'sync' / 'daily')
  EXCLUDE_SOURCES="MyBotApp AnotherApp"   # space/comma separated
  INCLUDE_REPLIES=1
  NO_QUOTES=1

  # --- Docs ingest (incremental) ---
  DOCS_PATH=docs/
  DOCS_OUT={{DEFAULT_OUT}}
  DOCS_STATE_FILE=state/docs_sync.json
  DOCS_MODE=style                # or qa_heading
  DOCS_LANG_HINT=""
  DOCS_MIN_CHARS=80
  DOCS_MAX_CHARS=1200
  DOCS_DEDUP_DATASET=1
  # DOCS_DELETE_MISSING=1

  # --- Unify (auto-merge JSONL) ---
  UNIFY_OUT={{DEFAULT_OUT}}      # default output when not provided
  UNIFY_SHUFFLE=1                # set 0 to disable
  UNIFY_SEED=13
  UNIFY_EXCLUDE=dataset/train_eval.jsonl,**/scratch.jsonl

  # --- Python / paths ---
  PY_BIN=python3.11
  VENV_DIR=.venv
  REQ_FILE=requirements.txt
  DEFAULT_OUT={{DEFAULT_OUT}}
  STATE_FILE={{STATE_FILE}}
  TRAIN_SCRIPT={{TRAIN_SCRIPT}}
  USAGE_FILE=usage.txt

Notes:
- `convert` writes a fresh base file (use for first import).
- `sync` appends only new tweets (tracked by state).
- `docs_sync` and `sql_threads` create separate JSONL files—run `unify` to merge all sources into a single training file.
- `unify` auto-scans dataset/*.jsonl unless you pass explicit --in files.

